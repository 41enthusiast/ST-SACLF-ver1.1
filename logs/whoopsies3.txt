/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: mridulav (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.11
wandb: Run data is saved locally in /home2/txlx81/new_repos/kaokore-visapp/wandb/run-20221019_200639-3irh37jq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-blaze-66
wandb: ⭐️ View project at https://wandb.ai/mridulav/stcluster-classifier
wandb: 🚀 View run at https://wandb.ai/mridulav/stcluster-classifier/runs/3irh37jq
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  rank_zero_deprecation(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 42
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type    | Params
----------------------------------
0 | model | AttnVGG | 21.2 M
----------------------------------
1.2 M     Trainable params
20.0 M    Non-trainable params
21.2 M    Total params
84.704    Total estimated model params size (MB)
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/nn/functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. 
  warnings.warn(
wandb: Waiting for W&B process to finish... (success).
wandb: - 1.746 MB of 1.746 MB uploaded (0.000 MB deduped)wandb: \ 1.746 MB of 1.746 MB uploaded (0.000 MB deduped)wandb: | 1.746 MB of 1.746 MB uploaded (0.000 MB deduped)wandb: / 1.746 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: - 1.746 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: \ 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: | 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: / 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: - 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: \ 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: | 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb: / 1.760 MB of 1.758 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       test_accuracy ▁
wandb:             test_f1 ▁
wandb:           test_loss ▁
wandb:      test_precision ▁
wandb:         test_recall ▁
wandb:     train_acc_epoch ▁▄▅▅▆▆▆▆▇▇▇▇▇█▇▇█▇██
wandb:      train_acc_step ▄▇▅▅▇▅▇▄█▄▅█▅▇█▇▁▂▅▅▂█▇▄▅▇▇▇▅▄▇▂▂▅▇▇▅█▇▄
wandb:    train_loss_epoch █▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁
wandb:     train_loss_step █▂▂▄▃▃▃▅▁▆▇▁█▂▁▄▆▅▄▃▄▂▃▄▃▃▃▂▂▄▃▅▅▄▁▂▅▁▂█
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb:             val_acc ▇▁▃▇▇█▇▇█▇▇▇█▆▇▇███▇
wandb:        val_accuracy ▇▁▃▇▇█▇▇█▇▇▇█▆▇▇███▇
wandb:              val_f1 ▆▁▃▇▃█▅▅▆▆▅▅▇▇▅▃▆▆▇█
wandb:            val_loss ▂▆█▂▃▁▁▂▁▂▂▂▂▃▂▂▂▁▂▂
wandb:       val_precision ▅▁▅▆▂▇▄▄▆▆▅▄▇█▅▃▅▄▆█
wandb:          val_recall ▆▁▂▇▂█▄▃▅▅▄▄▇▇▅▂▅▅▇█
wandb: 
wandb: Run summary:
wandb:               epoch 20
wandb:             lr-Adam 0.001
wandb:       test_accuracy 0.64953
wandb:             test_f1 0.41356
wandb:           test_loss 1.35696
wandb:      test_precision 0.50393
wandb:         test_recall 0.36986
wandb:     train_acc_epoch 0.80573
wandb:      train_acc_step 0.75
wandb:    train_loss_epoch 0.81848
wandb:     train_loss_step 0.82832
wandb: trainer/global_step 33780
wandb:             val_acc 0.66864
wandb:        val_accuracy 0.66769
wandb:              val_f1 0.51543
wandb:            val_loss 1.23637
wandb:       val_precision 0.59996
wandb:          val_recall 0.48346
wandb: 
wandb: Synced ethereal-blaze-66: https://wandb.ai/mridulav/stcluster-classifier/runs/3irh37jq
wandb: Synced 5 W&B file(s), 26 media file(s), 28 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221019_200639-3irh37jq/logs
