/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
wandb: Currently logged in as: mridulav (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.11
wandb: Run data is saved locally in /home2/txlx81/new_repos/kaokore-visapp/wandb/run-20221020_013946-1dou9y4a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-microwave-72
wandb: ⭐️ View project at https://wandb.ai/mridulav/stcluster-classifier
wandb: 🚀 View run at https://wandb.ai/mridulav/stcluster-classifier/runs/1dou9y4a
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  rank_zero_deprecation(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Global seed set to 42
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name  | Type    | Params
----------------------------------
0 | model | AttnVGG | 15.9 M
----------------------------------
1.2 M     Trainable params
14.7 M    Non-trainable params
15.9 M    Total params
63.465    Total estimated model params size (MB)
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home2/txlx81/new_repos/mv_test1/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
wandb: Waiting for W&B process to finish... (success).
wandb: - 2.159 MB of 2.241 MB uploaded (0.000 MB deduped)wandb: \ 2.241 MB of 2.241 MB uploaded (0.000 MB deduped)wandb: | 2.241 MB of 2.241 MB uploaded (0.000 MB deduped)wandb: / 2.241 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: - 2.241 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: \ 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: | 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: / 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: - 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: \ 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: | 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: / 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb: - 2.254 MB of 2.254 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:               epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████
wandb:             lr-Adam ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:       test_accuracy ▁
wandb:             test_f1 ▁
wandb:           test_loss ▁
wandb:      test_precision ▁
wandb:         test_recall ▁
wandb:     train_acc_epoch ▁▃▄▄▅▅▆▆▆▇▇▇▇▇██████
wandb:      train_acc_step ▃▆▁▃▃▃▆█▁▃███▆▃▁███████▆▆▁▆▆██████▆▆█▆█▆
wandb:    train_loss_epoch █▆▅▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▂▁
wandb:     train_loss_step ▄▃█▄▄▃▂▁▄▃▂▁▁▂▄▄▁▂▂▁▁▁▁▄▁▃▂▁▁▁▁▁▁▁▂▂▁▃▁▂
wandb: trainer/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:             val_acc ▅▇▆█▇▇▇▆▇▁▇▆▇▆▆▅▅▅▅▇
wandb:        val_accuracy ▅▇▆█▇▇▇▆▆▁▇▆▆▆▆▅▅▅▅▇
wandb:              val_f1 ▄█▄█▆▆█▆▅▁▇▅▅▄▆▄▃▃▄▅
wandb:            val_loss ▂▁▂▁▂▂▄▃▃▇▇▅▅▆▆█▆▆▇▆
wandb:       val_precision ▄▇▃▇▅▆█▅▅▁█▄▅▃▆▃▂▂▄▅
wandb:          val_recall ▄█▄█▆▆█▆▅▁▇▅▅▄▆▄▃▃▄▅
wandb: 
wandb: Run summary:
wandb:               epoch 20
wandb:             lr-Adam 0.001
wandb:       test_accuracy 0.76439
wandb:             test_f1 0.54833
wandb:           test_loss 0.80541
wandb:      test_precision 0.60613
wandb:         test_recall 0.52071
wandb:     train_acc_epoch 0.93872
wandb:      train_acc_step 1.0
wandb:    train_loss_epoch 0.37019
wandb:     train_loss_step 0.24951
wandb: trainer/global_step 16900
wandb:             val_acc 0.81302
wandb:        val_accuracy 0.81297
wandb:              val_f1 0.59711
wandb:            val_loss 1.08054
wandb:       val_precision 0.64387
wandb:          val_recall 0.56963
wandb: 
wandb: Synced dandy-microwave-72: https://wandb.ai/mridulav/stcluster-classifier/runs/1dou9y4a
wandb: Synced 5 W&B file(s), 26 media file(s), 28 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221020_013946-1dou9y4a/logs
